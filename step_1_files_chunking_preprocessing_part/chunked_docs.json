[{"filename": "databricks-certified-data-engineer-associate-exam-guide.pdf", "content": "databricks exam guide databricks certi\ufb01ed data engineer associate provide exam guide feedback purpose of this exam guide the purpose of this exam guide is to give you an overview of the exam and what is covered on the exam to help you determine your exam readiness this document will get updated anytime there are any changes to an exam and when those changes will take effect on an exam so that you can be prepared this version covers the currently live exam as of april 1 2024 please check back two weeks before you take your exam to make sure you have the most current version audience description the databricks certi\ufb01ed data engineer associate certi\ufb01cation exam assesses an individual\u2019s ability to use the databricks lakehouse platform to complete introductory data engineering tasks this includes an understanding of the lakehouse platform and its workspace its architecture and its capabilities it also assesses the ability to perform multihop architecture etl tasks using apache spark sql and python in both batch and incrementally processed paradigms finally the exam assesses the tester\u2019s ability to put basic etl pipelines and databricks sql queries and dashboards into production while maintaining entity permissions individuals who pass this certi\ufb01cation exam can be expected to complete basic data engineering tasks using databricks and its associated tools about the exam \u25cf number of items 45 multiplechoice questions \u25cf time limit 90 minutes \u25cf registration fee usd 200 plus applicable taxes as required per local law \u25cf delivery method online proctored \u25cf test aides none allowed \u25cf prerequisite none required course attendance and six months of handson experience in databricks is highly recommended \u25cf validity 2 years \u25cf recerti\ufb01cation recerti\ufb01cation is required every two years to maintain your certi\ufb01ed status to recertify you must take the full exam that is currently live please review the \u201cgetting ready for the exam\u201d section on the exam webpage to prepare for taking the exam again\u25cf unscored content exams may include unscored items to gather statistical information for future use these items are not identi\ufb01ed on the form and do not impact your score additional time is factored into account for this content recommended training \u25cf instructorled data engineering with databricks \u25cf selfpaced data engineering with databricks available in databricks academy exam outline section 1 databricks lakehouse platform \u25cf describe the relationship between the data lakehouse and the data warehouse \u25cf identify the improvement in data quality in the data lakehouse over the data lake \u25cf compare and contrast silver and gold tables which workloads will use a bronze table as a source which workloads will use a gold table as a source \u25cf identify elements of the databricks platform architecture such as what is located in the data plane versus the control plane and what resides in the customer\u2019s cloud account \u25cf differentiate between allpurpose clusters and jobs clusters \u25cf identify how cluster software is versioned using the databricks runtime \u25cf identify how clusters can be \ufb01ltered to view those that are accessible by the user \u25cf describe how clusters are terminated and the impact of"}, {"filename": "databricks-certified-data-engineer-associate-exam-guide.pdf", "content": "terminating a cluster \u25cf identify a scenario in which restarting the cluster will be useful \u25cf describe how to use multiple languages within the same notebook \u25cf identify how to run one notebook from within another notebook \u25cf identify how notebooks can be shared with others \u25cf describe how databricks repos enables cicd work\ufb02ows in databricks \u25cf identify git operations available via databricks repos \u25cf identify limitations in databricks notebooks version control functionality relative to repos section 2 elt with apache spark \u25cf extract data from a single \ufb01le and from a directory of \ufb01les \u25cf identify the pre\ufb01x included after the from keyword as the data type \u25cf create a view a temporary view and a cte as a reference to a \ufb01le \u25cf identify that tables from external sources are not delta lake tables \u25cf create a table from a jdbc connection and from an external csv \ufb01le \u25cf identify how the countif function and the count where x is null can be used \u25cf identify how the countrow skips null values \u25cf deduplicate rows from an existing delta lake table\u25cf create a new table from an existing table while removing duplicate rows \u25cf deduplicate a row based on speci\ufb01c columns \u25cf validate that the primary key is unique across all rows \u25cf validate that a \ufb01eld is associated with just one unique value in another \ufb01eld \u25cf validate that a value is not present in a speci\ufb01c \ufb01eld \u25cf cast a column to a timestamp \u25cf extract calendar data from a timestamp \u25cf extract a speci\ufb01c pattern from an existing string column \u25cf utilize the dot syntax to extract nested data \ufb01elds \u25cf identify the bene\ufb01ts of using array functions \u25cf parse json strings into structs \u25cf identify which result will be returned based on a join query \u25cf identify a scenario to use the explode function versus the \ufb02atten function \u25cf identify the pivot clause as a way to convert data from wide format to a long format \u25cf de\ufb01ne a sql udf \u25cf identify the location of a function \u25cf describe the security model for sharing sql udfs \u25cf use casewhen in sql code \u25cf leverage casewhen for custom control \ufb02ow section 3 incremental data processing \u25cf identify where delta lake provides acid transactions \u25cf identify the bene\ufb01ts of acid transactions \u25cf identify whether a transaction is acidcompliant \u25cf compare and contrast data and metadata \u25cf compare and contrast managed and external tables \u25cf identify a scenario to use an external table \u25cf create a managed table \u25cf identify the location of a table \u25cf inspect the directory structure of delta lake \ufb01les \u25cf identify who has written previous versions of a table \u25cf review a history of table transactions \u25cf roll back a table to a previous version \u25cf identify that a table can be rolled back to a previous version \u25cf query a speci\ufb01c version of a table \u25cf identify why zordering is bene\ufb01cial to delta lake tables \u25cf identify how vacuum commits deletes \u25cf identify the kind of \ufb01les optimize compacts \u25cf identify ctas"}, {"filename": "databricks-certified-data-engineer-associate-exam-guide.pdf", "content": "as a solution \u25cf create a generated column \u25cf add a table comment\u25cf use create or replace table and insert overwrite \u25cf compare and contrast create or replace table and insert overwrite \u25cf identify a scenario in which merge should be used \u25cf identify merge as a command to deduplicate data upon writing \u25cf describe the bene\ufb01ts of the merge command \u25cf identify why a copy into statement is not duplicating data in the target table \u25cf identify a scenario in which copy into should be used \u25cf use copy into to insert data \u25cf identify the components necessary to create a new dlt pipeline \u25cf identify the purpose of the target and of the notebook libraries in creating a pipeline \u25cf compare and contrast triggered and continuous pipelines in terms of cost and latency \u25cf identify which source location is utilizing auto loader \u25cf identify a scenario in which auto loader is bene\ufb01cial \u25cf identify why auto loader has inferred all data to be string from a json source \u25cf identify the default behavior of a constraint violation \u25cf identify the impact of on violation drop row and on violation fail updatefor a constraint violation \u25cf explain change data capture and the behavior of apply changes into \u25cf query the events log to get metrics perform audit loggin examine lineage \u25cf troubleshoot dlt syntax identify which notebook in a dlt pipeline produced an error identify the need for live in create statement identify the need for stream in from clause section 4 production pipelines \u25cf identify the bene\ufb01ts of using multiple tasks in jobs \u25cf set up a predecessor task in jobs \u25cf identify a scenario in which a predecessor task should be set up \u25cf review a tasks execution history \u25cf identify cron as a scheduling opportunity \u25cf debug a failed task \u25cf set up a retry policy in case of failure \u25cf create an alert in the case of a failed task \u25cf identify that an alert can be sent via email section 5 data governance \u25cf identify one of the four areas of data governance \u25cf compare and contrast metastores and catalogs \u25cf identify unity catalog securables \u25cf de\ufb01ne a service principal \u25cf identify the cluster security modes compatible with unity catalog \u25cf create a ucenabled allpurpose cluster \u25cf create a dbsql warehouse\u25cf identify how to query a threelayer namespace \u25cf implement data object access control \u25cf identify colocating metastores with a workspace as best practice \u25cf identify using service principals for connections as best practice \u25cf identify the segregation of business units across catalog as best practice sample questions these questions are retired from a previous version of the exam the purpose is to show you objectives as they are stated on the exam guide and give you a sample question that aligns to the objective the exam guide lists the objectives that could be covered on an exam the best way to prepare for a certi\ufb01cation exam is to review the exam outline in the exam guide question 1 objective describe the bene\ufb01ts of a data"}, {"filename": "databricks-certified-data-engineer-associate-exam-guide.pdf", "content": "lakehouse over a traditional data warehouse what is a bene\ufb01t of a data lakehouse that is unavailable in a traditional data warehouse a a data lakehouse provides a relational system of data management b a data lakehouse captures snapshots of data for version control purposes c a data lakehouse couples storage and compute for complete control d a data lakehouse utilizes proprietary storage formats for data e a data lakehouse enables both batch and streaming analytics question 2 objective identify query optimization techniques a data engineering team needs to query a delta table to extract rows that all meet the same condition however the team has noticed that the query is running slowly the team has already tuned the size of the data \ufb01les upon investigating the team has concluded that the rows meeting the condition are sparsely located throughout each of the data \ufb01les which optimization techniques could speed up the query a data skipping b zordering c binpacking d write as a parquet \ufb01le e tuning the \ufb01le sizequestion 3 objective identify data workloads that utilize a silver table as its source which data workload will utilize a silver table as its source a a job that enriches data by parsing its timestamps into a humanreadable format b a job that queries aggregated data that already feeds into a dashboard c a job that ingests raw data from a streaming source into the lakehouse d a job that aggregates cleaned data to create standard summary statistics e a job that cleans data by removing malformatted records question 4 objective describe how to con\ufb01gure a refresh schedule an engineering manager uses a databricks sql query to monitor their team\u2019s progress on \ufb01xes related to customerreported bugs the manager checks the results of the query every day but they are manually rerunning the query each day and waiting for the results how should the query be scheduled to ensure the results of the query are updated each day a to refresh every 12 hours from the query\u2019s page in databricks sql b to refresh every 1 day from the query\u2019s page in databricks sql c to run every 12 hours from the jobs ui d to refresh every 1 day from the sql warehouse page in databricks sql e to refresh every 12 hours from the sql warehouse page in databricks sql question 5 objective identify commands for granting appropriate permissions a new data engineer has started at a company the data engineer has recently been added to the company\u2019s databricks workspace as newengineercompanycom the data engineer needs to be able to query the table sales in the database retail the new data engineer already has been granted usage on the database retailwhich command should be used to grant the appropriate permissions to the new data engineer a grant usage on table sales to newengineercompanycom b grant create on table sales to newengineercompanycom c grant select on table sales to newengineercompanycom d grant usage on table newengineercompanycom to sales e grant select on table newengineercompanycom to sales answers question 1 e question"}, {"filename": "databricks-certified-data-engineer-associate-exam-guide.pdf", "content": "2 b question 3 d question 4 b question 5 c"}, {"filename": "AWS-Certified-Data-Engineer-Associate_Exam-Guide (1).pdf", "content": "version 10 dea c01 1 page aws certified data engineer associate dea c01 exam guide introduction the aws certified data engineer associate deac01 exam validates a candidate\u2019s ability to implement data pipelines and to monitor troubleshoot and optimize cost and performance issues in accordance with best practices the exam also validates a candidate\u2019s ability to complete the following task s \u2022 ingest and transform data and orchestrate data pipelines whi le applying programming concepts \u2022 choose an optimal data store design data models catalog data schemas and manage data lifecycles \u2022 operationalize maintain and monitor data pipelines analyze data and ensure data quality \u2022 implement appropriate authentication authorization data encryption privacy and governance enable logging target candidate description the target candidate should have the equivalent of 2 \u20133 years of experience in data engineering the target candidate should understand the effects of volume variety and velocity on data ingestion transformation modeling security governance privacy schema design and optimal data store design additionally the target candidate should have a t least 1\u20132 years of hands on experience wit h aws services recommended general it knowledge the target candidate should have the following general it knowledge \u2022 setup and maintenance of extract transform and load etl pipelines from ingestion to destination \u2022 application of high level but language agnostic programming concepts as required by the pipeline \u2022 how to use git commands for source control \u2022 how to use data lakes to store data \u2022 general concepts for networking storage and compute version 10 dea c01 2 page recommended aws knowledge the target candidate should have the following aws knowledge \u2022 how to use aws services to accomplish the tasks listed in the introduction section of this exam guide \u2022 an understanding of the aws services for encryption governance protection and logging of all data that is part of data pi pelines \u2022 the ability to compare aws services to understand the cost performance and functional differences between services \u2022 how to structure sql queries and how to run sql queries on aws services \u2022 an understanding of how to analyze data verify data qualit y and ensure data consistency by using aws services job tasks that are out of scope for the target candidate the following list contains job tasks that the target candidate is not expected to be able to perform this list is non exhaustive these tasks are out of scope for the exam \u2022 perform artificial intelligence and machine learning aiml tasks \u2022 demonstrate knowledge of programming language specific syntax \u2022 draw business conclusions based on data refer to the appendix for a list of in scope aws services and features and a list of outofscope aws services and features exam content response types there are two types of questions on the exam \u2022 multiple choice has one correct response and three incorrect responses distractors \u2022 multiple response has two or more correct responses out of five or more response options version 10 dea c01 3 page select one or more responses that best complete the statement or answer the"}, {"filename": "AWS-Certified-Data-Engineer-Associate_Exam-Guide (1).pdf", "content": "question distractors or incorrect answers are response options that a candidate with incomplete knowledge or skill might choose distractors are generally plausible responses that match the content area unanswered questions are scored as incorrect there is no penalty for guessing the exam include s 50 questions that affect your scor e unscored content the exam include s 15 unscored questions that do not affect your score aws collects information about performance on these unscored questions to evaluate these questions for future use as scored questions these unscored questions are not identified on the exam exam results the aws certified data engineer associate deac01 exam has a pass or fail designation the exam is scored against a minimum standard established by aws professionals who follow certification industry best p ractices and guidelines your results for the exam are reported as a scaled score of 100\u20131000 the minimum passing score is 720 your score shows how you performed on the exam as a whole and whether you passed scaled scoring models help equate scores across multiple exam forms that might have slightly different difficulty levels your score report could contain a table of classifications of your performance at each section level the exam uses a compensatory scoring model which means that you do not n eed to achieve a passing score in each section you need to pass only the overall exam each section of the exam has a specific weighting so some sections have more questions than other sections have the table of classifications contains general informat ion that highlights your strengths and weaknesses use caution when you interpret section level feedback version 10 dea c01 4 page content outline this exam guide includes weightings content domains and task statements for the exam this guide does not provide a comprehensive list of the content on the exam however additional context for each task statement is available to help you prepare for the exam the exam has the following content domains and weightings \u2022 domain 1 data ingestion and transformation 34 of scored content \u2022 domain 2 data store management 26 of scored content \u2022 domain 3 data operations and support 22 of scored content \u2022 domain 4 data security and governance 18 of scored content domain 1 data ingestion and transformation task state ment 11 perform data ingestion knowledge of \u2022 throughput and latency characteristics for aws services that ingest data \u2022 data ingestion patterns for example frequency and data history \u2022 streaming data ingestion \u2022 batch data ingestion for example scheduled ingestion event driven ingestion \u2022 replayability of data ingestion pipelines \u2022 stateful and stateless data transactions skills in \u2022 reading data from st reaming sources for example amazon kinesis amazon managed streaming for apache kafka amazon msk amaz on dynamodb streams aws database migration service aws dms aws glue amazon redshift \u2022 reading data from batch sources for example amazon s3 aws glue amazon emr aws dms amazon redshift aws lambda amazon appflow \u2022 implementing appropriate configur ation options for batch ingestion \u2022 consuming data apis version 10 dea c01 5"}, {"filename": "AWS-Certified-Data-Engineer-Associate_Exam-Guide (1).pdf", "content": "page \u2022 setting up schedulers by using amazon eventbridge apache airflow or time based schedules for jobs and crawlers \u2022 setting up event triggers for example amazon s3 event notifications eventbridge \u2022 calling a lambda function from amazon kinesis \u2022 creating allowlists for ip addresses to allow connections to data sources \u2022 implementing throttling and overcoming rate limits for example dynamodb amazon rds kinesis \u2022 managing fan in and fan out for streaming dat a distribution task statement 12 transform and process data knowledge of \u2022 creation of etl pipelines based on business requirements \u2022 volume velocity and variety of data for example structured data unstructured data \u2022 cloud computing and distributed co mputing \u2022 how to use apache spark to process data \u2022 intermediate data staging locations skills in \u2022 optimizing container usage for performance needs for example amazon elastic kubernetes service amazon eks amazon elastic container service amazon ecs \u2022 connecting to different data sources for example java database connectivity jdbc open database connectivity odbc \u2022 integrating data from multiple sources \u2022 optimizing costs while processing data \u2022 implementing data transformation services based on requirements for example amazon emr aws glue lambda amazon redshift \u2022 transforming data between formats for example from csv to apache parquet \u2022 troubleshooting and debugging common transformation failures and performance issues \u2022 creating data apis to make data available to other systems by using aws services version 10 dea c01 6 page task statement 13 orchestrate data pipelines knowledge of \u2022 how to integrate various aws services to create etl pipelines \u2022 event driven architecture \u2022 how to configure aws services for data pipelin es based on schedules or dependencies \u2022 serverless workflows skills in \u2022 using orchestration services to build workflows for data etl pipelines for example lambda eventbridge amazon managed workflows for apache airflow amazon mwaa aws step functions a ws glue workflows \u2022 building data pipelines for performance availability scalability resiliency and fault tolerance \u2022 implementing and maintaining serverless workflows \u2022 using notification services to send alerts for example amazon simple notification ser vice amazon sns amazon simple queue service amazon sqs task statement 14 apply programming concepts knowledge of \u2022 continuous integration and continuous delivery cicd implementation testing and deployment of data pipelines \u2022 sql queries for d ata source queries and data transformations \u2022 infrastructure as code iac for repeatable deployments for example aws cloud development kit aws cdk aws cloudformation \u2022 distributed computing \u2022 data structures and algorithms for example graph data struct ures and tree data structures \u2022 sql query optimization version 10 dea c01 7 page skills in \u2022 optimizing code to reduce runtime for data ingestion and transformation \u2022 configuring lambda functions to meet concurrency and performance needs \u2022 performing sql queries to transform data for example amazon redshift stored procedures \u2022 structuring sql queries to meet data pipeline requirements \u2022 using git commands to perform actions such as creating updating cloning and branching repositories \u2022 using the aws serverless application model aws sam to package and deploy serverless data pipelines for example lambda functions step"}, {"filename": "AWS-Certified-Data-Engineer-Associate_Exam-Guide (1).pdf", "content": "functions dynamodb tables \u2022 using and mounting storage volumes from within lambda functions domain 2 data store management task statement 21 choose a data store know ledge of \u2022 storage platforms and their characteristics \u2022 storage services and configurations for specific performance demands \u2022 data storage formats for example csv txt parquet \u2022 how to align data storage with data migration requirements \u2022 how to determine t he appropriate storage solution for specific access patterns \u2022 how to manage locks to prevent access to data for example amazon redshift amazon rds skills in \u2022 implementing the appropriate storage services for specific cost and performance requirements f or example amazon redshift amazon emr aws lake formation amazon rds dynamodb amazon kinesis data streams amazon msk \u2022 configuring the appropriate storage services for specific access patterns and requirements for example amazon redshift amazon emr lake formation amazon rds dynamodb version 10 dea c01 8 page \u2022 applying storage services to appropriate use cases for example amazon s3 \u2022 integrating migration tools into data processing systems for example aws transfer family \u2022 implementing data migration or remote access methods for example amazon redshift federated queries amazon redshift materialized views amazon redshift spectrum task statement 22 understand data cataloging systems knowledge of \u2022 how to create a data catalog \u2022 data classification based on requireme nts \u2022 components of metadata and data catalogs skills in \u2022 using data catalogs to consume data from the data\u2019s source \u2022 building and referencing a data catalog for example aws glue data catalog apache hive metastore \u2022 discovering schemas and using aws glue cr awlers to populate data catalogs \u2022 synchronizing partitions with a data catalog \u2022 creating new source or target connections for cataloging for example aws glue task statement 23 manage the lifecycle of data knowledge of \u2022 appropriate storage solutions to address hot and cold data requirements \u2022 how to optimize the cost of storage based on the data lifecycle \u2022 how to delete data to meet business and legal requirements \u2022 data retention policies and archiving strategies \u2022 how to protect data with appropriate resilien cy and availability version 10 dea c01 9 page skills in \u2022 performing load and unload operations to move data between amazon s3 and amazon redshift \u2022 managing s3 lifecycle policies to change the storage tier of s3 data \u2022 expiring data when it reaches a specific age by using s3 lifecycl e policies \u2022 managing s3 versioning and dynamodb ttl task statement 24 design data models and schema evolution knowledge of \u2022 data modeling concepts \u2022 how to ensure accuracy and trustworthiness of data by using data lineage \u2022 best practices for indexing partitioning strategies compression and other data optimization techniques \u2022 how to model structured semi structured and unstructured data \u2022 schema evolution techniques skills in \u2022 designing schemas for amazon redshift dynamodb and lake formation \u2022 addressi ng changes to the characteristics of data \u2022 performing schema conversion for example by using the aws schema conversion tool aws sct and aws dms schema conversion \u2022 establishing data lineage"}, {"filename": "AWS-Certified-Data-Engineer-Associate_Exam-Guide (1).pdf", "content": "by using aws tools for example amazon sagemaker ml lineage tra cking domain 3 data operations and support task statement 31 automate data processing by using aws services knowledge of \u2022 how to maintain and troubleshoot data processing for repeatable business outcomes \u2022 api calls for data processing \u2022 which services ac cept scripting for example amazon emr amazon redshift aws glue version 10 dea c01 10 page skills in \u2022 orchestrating data pipelines for example amazon mwaa step functions \u2022 troubleshooting amazon managed workflows \u2022 calling sdks to access amazon features from code \u2022 using the fea tures of aws services to process data for example amazon emr amazon redshift aws glue \u2022 consuming and maintaining data apis \u2022 preparing data transformation for example aws glue databrew \u2022 querying data for example amazon athena \u2022 using lambda to automate data processing \u2022 managing events and schedulers for example eventbridge task statement 32 analyze data by using aws services knowledge of \u2022 tradeoffs between provisioned services and serverless services \u2022 sql queries for example select stateme nts with multiple qualifiers or join clauses \u2022 how to visualize data for analysis \u2022 when and how to apply cleansing techniques \u2022 data aggregation rolling average grouping and pivoting skills in \u2022 visualizing data by using aws services and tools for example aws glue databrew amazon quicksight \u2022 verifying and cleaning data for example lambda athena quicksight jupyter notebooks amazon sagemaker data wrangler \u2022 using athena to query data or to create views \u2022 using athena notebooks that use apache spark to exp lore data task statement 33 maintain and monitor data pipelines knowledge of \u2022 how to log application data \u2022 best practices for performance tuning \u2022 how to log access to aws services \u2022 amazon macie aws cloudtrail and amazon cloudwatch version 10 dea c01 11 page skills in \u2022 extracting logs for audits \u2022 deploying logging and monitoring solutions to facilitate auditing and traceability \u2022 using notifications during monitoring to send alerts \u2022 troubleshooting performance issues \u2022 using cloudtrail to track api calls \u2022 troubleshooting and maintaining p ipelines for example aws glue amazon emr \u2022 using amazon cloudwatch logs to log application data with a focus on configuration and automation \u2022 analyzing logs with aws services for example athena amazon emr amazon opensearch service cloudwatch logs i nsights big data application logs task statement 34 ensure data quality knowledge of \u2022 data sampling techniques \u2022 how to implement data skew mechanisms \u2022 data validation data completeness consistency accuracy and integrity \u2022 data profiling skills in \u2022 running data quality checks while processing the data for example checking for empty fields \u2022 defining data quality rules for example aws glue databrew \u2022 investigating data consistency for example aws glue databrew domain 4 data security and governanc e task statement 41 apply authentication mechanisms knowledge of \u2022 vpc security networking concepts \u2022 differences between managed services and unmanaged services \u2022 authentication methods password based certificate based and role based \u2022 differences between aws managed policies and customer managed policies version 10 dea c01"}, {"filename": "AWS-Certified-Data-Engineer-Associate_Exam-Guide (1).pdf", "content": "12 page skills in \u2022 updating vpc security groups \u2022 creating and updating iam groups roles endpoints and services \u2022 creating and rotating credentials for password management for example aws secrets manager \u2022 setting up iam roles for access for example lambda amazon api gateway aws cli cloudformation \u2022 applying iam policies to roles endpoints and services for example s3 access points aws privatelink task statement 42 apply authorization mechanisms knowledge of \u2022 authorization methods role based policy based tag based and attribute based \u2022 principle of least privilege as it applies to aws security \u2022 role based access control and expected access patterns \u2022 methods to protect data from unauthorized acc ess across services skills in \u2022 creating custom iam policies when a managed policy does not meet the needs \u2022 storing application and database credentials for example secrets manager aws systems manager parameter store \u2022 providing database users groups and roles access and authority in a database for example for amazon redshift \u2022 managing permissions through lake formation for amazon redshift amazon emr athena and amazon s3 task statement 43 ensure data encryption and masking knowledge of \u2022 data enc ryption options available in aws analytics services for example amazon redshift amazon emr aws glue \u2022 differences between client side encryption and server side encryption \u2022 protection of sensitive data \u2022 data anonymization masking and key salting version 10 dea c01 13 page skills in \u2022 applying data masking and anonymization according to compliance laws or company policies \u2022 using encryption keys to encrypt or decrypt data for example aws key management service aws kms \u2022 configuring encryption across aws account boundaries \u2022 enabling encryption in transit for data task statement 44 prepare logs for audit knowledge of \u2022 how to log application data \u2022 how to log access to aws services \u2022 centralized aws logs skills in \u2022 using cloudtrail to track api calls \u2022 using cloudwatch logs to store appl ication logs \u2022 using aws cloudtrail lake for centralized logging queries \u2022 analyzing logs by using aws services for example athena cloudwatch logs insights amazon opensearch service \u2022 integrating various aws services to perform logging for example amazon emr in cases of large volumes of log data task statement 45 understand data privacy and governance knowledge of \u2022 how to protect personally identifiable information pii \u2022 data sovereignty skills in \u2022 granting permissions for data sharing for example data sharing for amazon redshift \u2022 implementing pii identification for example macie with lake formation \u2022 implementing data privacy strategies to prevent backups or replications of data to disallowed aws regions \u2022 managing configuration changes that have occurred in an account for example aws config version 10 dea c01 14 page appendix inscope aws services and features the following list contains aws services and features that are in scope for the exam this list is nonexhaustive and is subject to change aws offerings appear in categories that align with the offerings\u2019 primary functions analytics \u2022 amazon athena \u2022 amazon emr \u2022 aws glue \u2022 aws glue databrew \u2022 aws lake"}, {"filename": "AWS-Certified-Data-Engineer-Associate_Exam-Guide (1).pdf", "content": "formation \u2022 amazon kinesis data firehose \u2022 amazon kinesis data streams \u2022 amazon managed service for apache flink \u2022 amazon managed streaming for apache kafka amazon msk \u2022 amazon opensearch service \u2022 amazon quicksight application integration \u2022 amazon appflow \u2022 amazon eventbridge \u2022 amazon managed workflows for apache airflow amazon mwaa \u2022 amazon simple notification service amazon sns \u2022 amazon simple queue service amazon sqs \u2022 aws step functions cloud financial management \u2022 aws budgets \u2022 aws cost explorer version 10 dea c01 15 page compute \u2022 aws batch \u2022 amazon ec2 \u2022 aws lambda \u2022 aws serverless application model aws sam containers \u2022 amazon elastic container registry amazon ecr \u2022 amazon elastic container service amazon ecs \u2022 amazon elastic kubernetes service amazon eks database \u2022 amazon documentdb with mongodb compatibility \u2022 amazon dynamodb \u2022 amazon keyspaces for apache cassandra \u2022 amazon memorydb for redis \u2022 amazon neptune \u2022 amazon rds \u2022 amazon redshift developer tools \u2022 aws cli \u2022 aws cloud9 \u2022 aws cloud development kit aws cdk \u2022 aws codebuild \u2022 aws codecommit \u2022 aws codedeploy \u2022 aws codepipeline frontend web and mobile \u2022 amazon api gateway machine learning \u2022 amazon sagemaker version 10 dea c01 16 page management and governance \u2022 aws cloudformation \u2022 aws cloudtrail \u2022 amazon cloudwatch \u2022 amazon cloudwatch logs \u2022 aws config \u2022 amazon managed grafana \u2022 aws systems manager \u2022 aws well architected tool migration and transfer \u2022 aws application discovery service \u2022 aws application migration service \u2022 aws database migration service aws dms \u2022 aws datasync \u2022 aws schema conversion tool aws sct \u2022 aws snow family \u2022 aws transfer family networking and content delivery \u2022 amazon cloudfront \u2022 aws privatelink \u2022 amazon route 53 \u2022 amazon vpc security identity and compliance \u2022 aws identity and access management iam \u2022 aws key management service aws kms \u2022 amazon macie \u2022 aws secrets manager \u2022 aws shield \u2022 aws waf version 10 dea c01 17 page storage \u2022 aws backup \u2022 amazon elastic block store amazon ebs \u2022 amazon elastic file system amazon efs \u2022 amazon s3 \u2022 amazon s3 glacier outofscope aws services and features the following list contains aws services and features that are out of scope for the exam this list is nonexhaustive and is subject to change aws offerings that are entirely unrelated to the target job roles for the exam are excluded from th is list analytics \u2022 amazon finspace business applications \u2022 alexa for business \u2022 amazon chime \u2022 amazon connect \u2022 amazon honeycode \u2022 aws iq \u2022 amazon workdocs \u2022 amazon workmail compute \u2022 aws app runner \u2022 aws elastic beanstalk \u2022 amazon lightsail \u2022 aws outposts \u2022 aws serverless application repository containers \u2022 red hat openshift service on aws rosa version 10 dea c01 18 page database \u2022 amazon timestream developer tools \u2022 aws fault injection simulator aws fis \u2022 aws x ray frontend web and mobile \u2022 aws amplify \u2022 aws appsync \u2022 aws device farm \u2022 amazon location service \u2022 amazon pinpoint \u2022 amazon simple email service amazon ses internet of things iot \u2022"}, {"filename": "AWS-Certified-Data-Engineer-Associate_Exam-Guide (1).pdf", "content": "freertos \u2022 aws iot 1 click \u2022 aws iot device defender \u2022 aws iot device management \u2022 aws iot events \u2022 aws iot fleetwise \u2022 aws iot roborunner \u2022 aws iot sitewise \u2022 aws iot twinmaker machine learning \u2022 amazon codewhisperer \u2022 amazon devops guru management and governance \u2022 aws activate \u2022 aws managed services ams version 10 dea c01 19 page media services \u2022 amazon elastic transcoder \u2022 aws elemental appliances and software \u2022 aws elemental mediaconnect \u2022 aws elemental mediaconvert \u2022 aws elemental medialive \u2022 aws elemental mediapackage \u2022 aws elemental mediastore \u2022 aws elemental mediatailor \u2022 amazon interactive video service amazon ivs \u2022 amazon nimble studio migration and transfer \u2022 aws mainframe modernization \u2022 aws migration hub storage \u2022 ec2 image builder survey how useful was this exam guide let us know by takin g our survey"}, {"filename": "DP-203_StudyGuide_ENU_FY23Q2a_vnext.pdf", "content": "exam dp 203 data engineering on microsoft azure 1 study guide for exam dp 203 data engineering on microsoft azure purpose of this document this study guide should help you understand what to expect on the exam and includes a summary of the topics the exam might cover and links to additional resources the information and materials in this document should help you focus your studies as you prepare for the exam useful links description review the skills measured as of february 6 2023 this list represents the skills measured after the date provided study this list if you plan to take the exam after that date review the skills measured prior to february 6 2023 study this list of skills if you take your exam prior to the date provided change log you can go directly to the change log if you want to see the changes that will be made on the date provided how to earn the certification some certifications only require passing one exam while others require passing multiple exams certification renewal microsoft associate expert and specialty certifications expire annually you can renew by passing a free online assessment on microsoft learn your microsoft learn profile connecting your certification profile to microsoft learn allows you to schedule and renew exams and share and print certificates exam scoring and score reports a score of 700 or greater is required to pass exam sandbox you can explore the exam environment by visiting our exam sandbox exam dp 203 data engineering on microsoft azure 2 useful links description request accommodations if you use ass istive devices require extra time or need modification to any part of the exam experience you can request an accommodation take a free practice assessment test your skills with practice questions to help you prepare for the exam updates to the exam our exams are updated periodically to reflect skills that are required to perform a role we have included two versions of the skills measured objectives depending on when you are taking the exam we always update the english language version of the exam first some exams are localized into other languages and those are updated approximately eight weeks after the english version is updated other available languages are listed in the schedule exam section of the exam details webpage if the exam isnt available in your preferred language you can request an additional 30 minutes to complete the exam note the bullets that follow each of the skills measured are intended to illustrate how we are assessing that skill related topics may be covered in the exam note most questions cover features that are general availability ga the exam may contain questions on preview features if those featu res are commonly used skills measured as of february 6 2023 candidates for this exam should have subject matter expertise in integrating transforming and consolidating data from various structured unstructured and streaming data systems into a suitable schema for building analytics solutions azure data engineers help stakeholders understand the data through exploration and they build and maintain"}, {"filename": "DP-203_StudyGuide_ENU_FY23Q2a_vnext.pdf", "content": "secure and compliant data processing pipelines by using different tools and techniques these professionals use various azure data services and frameworks to store and produce cleansed and enhanced datasets for analysis this data store can be designed with different architecture patterns based on business requirements including modern data warehouse mdw big data or lakehouse architecture azure data engineers also help to ensure that the operationalization of data pipelines and data stores are high performing efficient organized and reliable given a set of business requirements and constraints these professionals help to identify and troubleshoot operational and data quality issues they also design implement monitor and optimize data platforms to meet the data pipelines exam dp 203 data engineering on microsoft azure 3 candidates for this exam must have solid knowledge of data processin g languages including sql python and scala and they need to understand parallel processing and data architecture patterns they should be proficient in using azure data factory azure synapse analytics azure stream analytics azure event hubs azure d ata lake storage and azure databricks to create data processing solutions \u2022 design and implement data storage 15 \u201320 \u2022 develop data processing 40 \u201345 \u2022 secure monitor and optimize data storage and data processing 30 \u201335 design and implement data storage 15\u201320 implement a partition strategy \u2022 implement a partition strategy for files \u2022 implement a partition strategy for analytical workloads \u2022 implement a partition strategy for streaming workloads \u2022 implement a partition strategy for azure synapse analytics \u2022 identify when partitioning is needed in azure data lake storage gen2 design and implement the data exploration layer \u2022 create and execute queries by using a compute solution that leverages sql serverless and spark cluster \u2022 implement azure synapse analytics data base templates \u2022 recommend azure synapse analytics database templates \u2022 push new or updated data lineage to microsoft purview \u2022 browse and search metadata in microsoft purview data catalog develop data processing 40\u201345 ingest and transform data \u2022 design and implement incremental loads \u2022 transform data by using apache spark \u2022 transform data by using transact sql t sql \u2022 ingest and transform data by using azure synapse pipelines or azure data factory \u2022 transform data by using azure stream analytics \u2022 cleanse data \u2022 handl e duplicate data \u2022 handle missing data \u2022 handle late arriving data \u2022 split data \u2022 shred json \u2022 encode and decode data \u2022 configure error handling for a transformation exam dp 203 data engineering on microsoft azure 4 \u2022 normalize and denormalize values \u2022 perform data exploratory analysis develop a batch processing solution \u2022 develop batch processing solutions by using azure data lake storage azure databricks azure synapse analytics and azure data factory \u2022 use polybase to load data to a sql pool \u2022 implement azure synapse link and query the replicated data \u2022 create data pipeline s \u2022 scale resources \u2022 configure the batch size \u2022 create tests for data pipelines \u2022 integrate jupyter or python notebooks into a data pipeline \u2022 upsert data \u2022 revert data to a previous state \u2022 configure"}, {"filename": "DP-203_StudyGuide_ENU_FY23Q2a_vnext.pdf", "content": "exception handling \u2022 configure batch retention \u2022 read from and write to a delta lake develop a stream processing solution \u2022 create a stream processing solution by using stream analytics and azure event hubs \u2022 process data by using spark structured streaming \u2022 create windowed aggregates \u2022 handle schema drift \u2022 process time serie s data \u2022 process data across partitions \u2022 process within one partition \u2022 configure checkpoints and watermarking during processing \u2022 scale resources \u2022 create tests for data pipelines \u2022 optimize pipelines for analytical or transactional purposes \u2022 handle interruptions \u2022 configure exception handling \u2022 upsert data \u2022 replay archived stream data manage batches and pipelines \u2022 trigger batches \u2022 handle failed batch loads \u2022 validate batch loads exam dp 203 data engineering on microsoft azure 5 \u2022 manage data pipelines in azure data factory or azure synapse pipelines \u2022 schedule data pipelines in data factory or azure synapse pipelines \u2022 implement version control for pipeline artifacts \u2022 manage spark jobs in a pipeline secure monitor and optimize data storage and data processing 30\u2013 35 implement data security \u2022 implement data masking \u2022 encrypt data at rest and in motion \u2022 implement row level and column level security \u2022 implement azure role based access control rbac \u2022 implement posix like access control lists acls for data lake storage gen2 \u2022 implement a data retention policy \u2022 implement secure endpoints p rivate and public \u2022 implement resource tokens in azure databricks \u2022 load a dataframe with sensitive information \u2022 write encrypted data to tables or parquet files \u2022 manage sensitive information monitor data storage and data processing \u2022 implement logging used by azu re monitor \u2022 configure monitoring services \u2022 monitor stream processing \u2022 measure performance of data movement \u2022 monitor and update statistics about data across a system \u2022 monitor data pipeline performance \u2022 measure query performance \u2022 schedule and monitor pipeline tests \u2022 interpret azure monitor metrics and logs \u2022 implement a pipeline alert strategy optimize and troubleshoot data storage and data processing \u2022 compact small files \u2022 handle skew in data \u2022 handle data spill \u2022 optimize resource management \u2022 tune queries by using indexers \u2022 tune queries by using cache exam dp 203 data engineering on microsoft azure 6 \u2022 troubleshoot a failed spark job \u2022 troubleshoot a failed pipeline run including activities executed in external services study resources we recommend that you train and get hands on experience before you take the exam we offer self study options and classroom training as well as links to documentation community sites and videos study resources links to learning and documentation get trained choose from self paced learning paths and modules or take an instructor led course find d ocumentation azure data lake storage azure synapse a nalytics azure databricks data factory azure stream analytics event hubs azure monitor ask a question microsoft qa microsoft docs get community support analytics on azure techcommunity azure synapse analytics techcommunity follow microsoft learn microsoft learn microsoft tech community find a video exam readiness zone data exposed browse other microsoft learn shows"}, {"filename": "DP-203_StudyGuide_ENU_FY23Q2a_vnext.pdf", "content": "change log key to understanding the table the topic groups also known as functional groups are in bold t ypeface followed by the objectives within each group the table is a comparison between the two versions of the exam skills measured and the third column describes the extent of the changes skill area prior to february 6 2023 skill area as of february 6 2023 changes audience profile major exam dp 203 data engineering on microsoft azure 7 skill area prior to february 6 2023 skill area as of february 6 2023 changes design and implement data storage design and implement data storage of exam decreased design a data storage structure removed design a partition strategy implement a partition strategy major design and implement the data exploration layer added design the serving layer removed implement physical data storage structures removed implement logical data structures removed implement the serving layer removed design and develop data processing develop data processing of exam increased ingest and transform data ingest and transform data major design and develop a batch processing solution develop a batch processing solution major design and develop a stream processing solution develop a stream processing solution major manage batches and pipelines manage batches and pipelines minor design and implement data security removed design security for data policies and standards removed implement data security removed exam dp 203 data engineering on microsoft azure 8 skill area prior to february 6 2023 skill area as of february 6 2023 changes monitor and optimize data storage and data processing secure monitor and optimize data storage and data processing of exam increased implement data security added monitor data storage and data processing monitor data storage and data processing major optimize and troubleshoot data storage and data processing optimize and troubleshoot data storage and data processing major skills measured prior to february 6 2023 audience profile candidates for this exam should have subject matter expertise integrating transforming and consolidating data from various structured and unstructured data systems into a structure that is suitable for building analytics solutions azure data engineers help stakeholders understand the data through exploration and they build and maintain secure and compliant data processing pipelines by using different tools and techniques these professionals use various azure data services and languages to store and pr oduce cleansed and enhanced datasets for analysis azure data engineers also help ensure that data pipelines and data stores are high performing efficient organized and reliable given a set of business requirements and constraints they deal with unan ticipated issues swiftly and they minimize data loss they also design implement monitor and optimize data platforms to meet the data pipelines needs a candidate for this exam must have strong knowledge of data processing languages such as sql python or scala and they need to understand parallel processing and data architecture patterns \u2022 design and implement data storage 40 \u201345 \u2022 design and develop data processing 25 \u201330 \u2022 design and implement data security 10 \u201315 \u2022 monitor and optimize data storage and data processing 10 \u201315 design and implement data storage"}, {"filename": "DP-203_StudyGuide_ENU_FY23Q2a_vnext.pdf", "content": "40 \u201345 design a data storage structure \u2022 design an azure data lake solution exam dp 203 data engineering on microsoft azure 9 \u2022 recommend file types for storage \u2022 recommend file types for analytical queries \u2022 design for efficient querying \u2022 design for data pruning \u2022 design a folder structure that represents the levels of data transformation \u2022 design a distribution strategy \u2022 design a data archiving solution design a partition strategy \u2022 design a partition strategy for files \u2022 design a partition strategy for analytical workloads \u2022 design a partition strategy for efficiencyperformance \u2022 design a partition strategy for azure synapse analytics \u2022 identify when partitioning is needed in azure data lake storage gen2 design the serving layer \u2022 design star schemas \u2022 design slowly changing dimensions \u2022 design a dimensional hierarchy \u2022 design a solution for temporal data \u2022 design for incremental loading \u2022 design analytical stores \u2022 design metastores in azure synapse analytics and azure databricks implement physical data storage s tructures \u2022 implement compression \u2022 implement partitioning \u2022 implement sharding \u2022 implement different table geometries with azure synapse analytics pools \u2022 implement data redundancy \u2022 implement distributions \u2022 implement data archiving implement logical data structures \u2022 build a temporal data solution \u2022 build a slowly changing dimension \u2022 build a logical folder structure \u2022 build external tables \u2022 implement file and folder structures for efficient querying and data pruning exam dp 203 data engineering on microsoft azure 10 implement the serving layer \u2022 deliver data in a relational sta r \u2022 deliver data in parquet files \u2022 maintain metadata \u2022 implement a dimensional hierarchy design and develop data processing 25 \u201330 ingest and transform data \u2022 transform data by using apache spark \u2022 transform data by using transact sql \u2022 transform data by using data factory \u2022 transform data by using azure synapse pipelines \u2022 transform data by using stream analytics \u2022 cleanse data \u2022 split data \u2022 shred json \u2022 encode and decode data \u2022 configure error handling for the transformation \u2022 normalize and denormalize values \u2022 transform data by using scala \u2022 perform data exploratory analysis design and develop a batch processing solution \u2022 develop batch processing solutions by using data factory data lake spark azure synapse pipelines polybase and azure databricks \u2022 create data pipelines \u2022 design and implement incremental data loads \u2022 design and develop slowly changing dimensions \u2022 handle security and compliance requirements \u2022 scale resources \u2022 configure the batch size \u2022 design and create tests for data pipelines \u2022 integrate jupyterpython notebooks into a da ta pipeline \u2022 handle duplicate data \u2022 handle missing data \u2022 handle late arriving data \u2022 upsert data \u2022 regress to a previous state \u2022 design and configure exception handling exam dp 203 data engineering on microsoft azure 11 \u2022 configure batch retention \u2022 design a batch processing solution \u2022 debug spark jobs by using the spark ui design and develop a stream processing solution \u2022 develop a stream processing solution by using stream analytics azure databricks and azure event hubs \u2022 process data by using"}, {"filename": "DP-203_StudyGuide_ENU_FY23Q2a_vnext.pdf", "content": "spark structured streaming \u2022 monitor for performance and functional regressi ons \u2022 design and create windowed aggregates \u2022 handle schema drift \u2022 process time series data \u2022 process across partitions \u2022 process within one partition \u2022 configure checkpointswatermarking during processing \u2022 scale resources \u2022 design and create tests for data pipelines \u2022 optimize pipelines for analytical or transactional purposes \u2022 handle interruptions \u2022 design and configure exception handling \u2022 upsert data \u2022 replay archived stream data \u2022 design a stream processing solution manage batches and pipelines \u2022 trigger batches \u2022 handle failed batch loads \u2022 validate batch loads \u2022 manage data pipelines in data factorysynapse pipelines \u2022 schedule data pipelines in data factorysynapse pipelines \u2022 implement version control for pipeline artifacts \u2022 manage spark jobs in a pipeline design and implement data security 10 \u201315 design security for data policies and standards \u2022 design data encryption for data at rest and in transit \u2022 design a data auditing strategy \u2022 design a data masking strategy \u2022 design for data privacy exam dp 203 data engineering on microsoft azure 12 \u2022 design a data retention policy \u2022 design to pu rge data based on business requirements \u2022 design azure role based access control azure rbac and posix like access control list acl for data lake storage gen2 \u2022 design row level and column level security implement data security \u2022 implement data masking \u2022 encryp t data at rest and in motion \u2022 implement row level and column level security \u2022 implement azure rbac \u2022 implement posix like acls for data lake storage gen2 \u2022 implement a data retention policy \u2022 implement a data auditing strategy \u2022 manage identities keys and secrets a cross different data platform technologies \u2022 implement secure endpoints private and public \u2022 implement resource tokens in azure databricks \u2022 load a dataframe with sensitive information \u2022 write encrypted data to tables or parquet files \u2022 manage sensitive information monitor and optimize data storage and data processing 10 \u201315 monitor data storage and data processing \u2022 implement logging used by azure monitor \u2022 configure monitoring services \u2022 measure performance of data movement \u2022 monitor and update statistics about data across a system \u2022 monitor data pipeline performance \u2022 measure query performance \u2022 monitor cluster performance \u2022 understand custom logging options \u2022 schedule and monitor pipeline tests \u2022 interpret azure monitor metrics and logs \u2022 interpret a spark d irected acyclic graph dag optimize and troubleshoot data storage and data processing \u2022 compact small files \u2022 rewrite user defined functions udfs \u2022 handle skew in data exam dp 203 data engineering on microsoft azure 13 \u2022 handle data spill \u2022 tune shuffle partitions \u2022 find shuffling in a pipeline \u2022 optimize resource ma nagement \u2022 tune queries by using indexers \u2022 tune queries by using cache \u2022 optimize pipelines for analytical or transactional purposes \u2022 optimize pipeline for descriptive versus analytical workloads \u2022 troubleshoot a failed spark job \u2022 troubleshoot a failed pipeline run"}, {"filename": "sample_data_engineering_text_file.txt", "content": "\ufeffwhat does a data engineer do responsibilities are going to change depending on the company you work for and what seniority level you are at but generally speaking most data engineers do the following gather data requirements such as how long the data needs to be stored how it will be used and what people and systems need access to the data maintain metadata about the data such as what technology manages the data data documentation how the data is secured the source of the data and the ultimate owner of the data ensure security and governance for the data using centralized security controls like ldap encrypting the data and auditing access to the data store the data using specialized technologies that are optimized for the particular use of the data such as a relational database a nosql database or blob storage process data for specific needs using tools that access data from different sources transform and enrich the data summarize the data and store the data in the storage system what is data engineering modernday data engineering is a subset of software engineering that focuses on moving storing and structuring data for use in applications or reporting what is the difference between a data engineer and x data engineer uses a combination of software engineering best practices and database design to build scalable data pipelines data integrations and data models for use in applications and reports see getting started with data engineering for a list of toolsconcepts to learn bi engineerbi developer uses primarily gui tools like ssis to build etl pipelines build data models for reports and even may build the reports themselves this role is similar in many ways to a data engineer but differs in tooling and skillsets required it is sometimes seen as a pathway to becoming a data engineer backend engineer this type of engineer is usually responsible for writing server side scripts and apis that interact with an application although they integrate applications with data they do not usually build data pipelines or the models in the database for analysis and are instead more focused on the application side database administrator focuses on making sure the database and infrastructure is available and secure ie backing up data user management server usage they do not typically build within the database itself data analyst uses data for basic to medium complexity analysis and builds reports and dashboards this role will typically require a good grasp of sql and reporting tools but relies on an engineer for the data data scientist uses data for more indepth analysis and visualizations typically the work is more research based and sometimes one will work on a product like a machine learning model senior data engineer minimum skills required soft skills intermediateadvanced sql advanced data modeling advanced scripting language python java or scala advanced indexing query optimization advanced cloud platform amazon web services microsoft azure or google cloud platform advanced infrastructure as code beginner batch data processing advanced relational database advanced nonrelational database advanced online transaction processing intermediate online analytical processing intermediate"}, {"filename": "sample_data_engineering_text_file.txt", "content": "data pipeline advanced nice to have infrastructure as code advanced reporting tools tableau superset metabase beginnerintermediate this data governance guide the emergence of data governance directly correlated with massively increased quantities of data that is now considered standard only a few decades ago many companies managed a quantity of data that was reasonable to organize by a database administrator but with the increased popularity of using different data sources and streaming data as well as the sheer quantity of data that is available nowadays it is dangerous to leave data to a single person and expect them to properly handle everything data governance is a philosophy of data management that focuses on establishing responsibility for data throughout the complete lifecycle of data the important part here is that data governance is unfortunately not a set it and forget it system for managing data at least during the beginning stages of implementation contrary to most of the philosophical cores of data engineering completely automating data governance is not recommended a quality data governance framework involves a system of rules processes procedures and enforcement strategies to ensure that data is properly accounted for different kinds of data may require finetuned guidelines for example how one would manage hipaa compliant data is very different than how one would handle a spreadsheet keeping track of lunch expenses for a team bowling party additionally different teams and suborganizations may have different cultures that necessitate the tweaking of any framework to ensure a customized fit these challenges necessitate some manual guidance and buyin from policymakers at the top of an organization without actual enforcement data governance tends to be difficult to implement due to the natural high friction it tends to create data use and management are particularly difficult when coordinating within and between different units to allow for better delivery on the business side and more accountability on the security side there are typically multiple steps required to implement data governance effectively a common fivepronged approach involves the following stages inventorymapping planning education implementation and enforcement whenever data governance is mentioned a key piece that is misunderstood is what engaging in data governance entails even logging onto a computer or sending a work email is technically an example of data governance the key is identifying what the end state of data governance looks like for a particular organization and then setting a formal practice for managing organizational information with consistency inventory when compiling an inventory of assets there are two pieces to keep in mind a standard data inventory tends to be a completely documented repository of information resources that are owned by an organization including the associated metadata a data inventory is focused on understanding data and identifying risks that are posed due to any missingbroken dependencies or gaps when tailoring an inventory for data governance it can be helpful to have explicit data mappings associated with databases and data ownerssubject matter experts this allows for an open communication line between policymakers and gatekeepers of important data sets there are many use cases where data governance"}, {"filename": "sample_data_engineering_text_file.txt", "content": "is being embraced by organizations of all different kinds larger companies often have different teams with completely different views on data the analytics team might view it as a secret weapon while the security team might view it as a liability this can lead to conflicts when it comes to how to best manage and assign ownership of data government institutions on the other hand traditionally deal with a high level of employee turnover leading to entire databases and key institutional knowledge being lost in the offboarding process detailing documenting and accounting for risks and opportunities is a key piece of data inventory when preparing it for data governance planning once the data assets and personnel are identified for an organization building trust with database owners subject matter experts and policymakers within the organization is key since this is the planning stage it is difficult to actually enforce data governance and as such getting buyin at this stage is crucial a comprehensive plan requires interviews and meetings with key stakeholders and database ownerssubject matter experts to understand what the needs and wants of all the suborganizations require and how to reconcile these demands specific processes that need to be ironed out should be included in this phase as well for example for a state level organization data trusts tend to be very important it teams like to consolidate databases and implement uniform security education in order for people in organizations to fully embrace data governance a training and education program is necessary typically classroomstyle lectures tend to not be effective instead convincing business owners how data governance can bring value to the company and security admins that data governance increases the security and stability of the work implementation actually implementing the process tends to be more of a practice of execution as long as theres been a quality plan and strategy produced and a training program that has started tackling some of the silos that have been built progress will eventually be made the biggest indicator of success for the implementation stage will be how interested and engaged key stakeholders are the main processes for data governance are the query engine data catalog and policy engine policymakers create a policy that is applied to the data catalog which is consumed through the query engine enforcement enforcement is relatively simple if the infrastructure for the implementation is thorough by applying the correct policies that restrict user roles minimal enforcement is necessary the main enforcement that is required during the process is making sure subject matter experts policymakers and stakeholders are all engaged this is largely the job of chief policymakers to create a plan of accountability summary data governance is the practice of making data more of an asset rather than a liability the ability to know and be able to search through all data assets an organization owns is transformative data governance in modern architecture is centered around policymakers data consumers and subject matter experts subject matter experts own data resources which must be mapped and consolidated under a data catalog with"}, {"filename": "sample_data_engineering_text_file.txt", "content": "the associated metadata"}, {"filename": "README.md", "content": "h1the data engineering handbookh1 pthis repo has all the resources you need to become an amazing data engineerp h2getting startedh2 pif you are new to data engineering start by following this a hrefhttpsblogdataengineeriopthe2024breakingintodataengineering2024 breaking into data engineering roadmapap pfor more applied learning check out the a hrefprojectsmdprojectsa section for more handson examples check out the a hrefinterviewsmdinterviewsa section for more advice on how to pass data engineering interviews check out the a hrefbooksmdbooksa section for a list of high quality data engineering books check out the a hrefcommunitiesmdcommunitiesa section for a list of high quality data engineering communities to join check out the a hrefnewslettersmdnewslettera section to learn via email p h2resourcesh2 h3great a hrefbooksmdlist of over 25 booksah3 ptop 3 must read books are a hrefhttpswwwamazoncomfundamentalsdataengineeringrobustsystemsdp1098108302fundamentals of data engineeringa a hrefhttpswwwamazoncomdesigningdataintensiveapplicationsreliablemaintainabledp1449373321designing dataintensive applicationsa a hrefhttpswwwamazoncomdesigningmachinelearningsystemsproductionreadydp1098107969designing machine learning systemsap h3great a hrefcommunitiesmdlist of over 10 communities to joinah3 ptop mustjoin communities for de a hrefhttpsdiscordggjgumaxncakeczachly data engineering discorda a hrefhttpsdatatalksclubslackdata talks club slacka a hrefhttpswwwdataengineerthingsorgaboutusdata engineer things communityap ptop mustjoin communities for ml a hrefhttpsdiscordcominviteezzszrrzvtadalflow discorda a hrefhttpsdiscordggdzh728c5t3chip huyen mlops discordap h3companiesh3 ul liorchestration li lia hrefhttpswwwmageaimageali lia hrefhttpswwwastronomerioastronomerali lia hrefhttpswwwprefectioprefectali lia hrefhttpswwwdagsteriodagsterali lia hrefhttpsairbytecomairbyteali lia hrefhttpskestraiokestraa li lia hrefhttpswwwshipyardappcomshipyardali lia hrefhttpsgithubcomdagworksinchamiltonhamiltonali lidata lake cloudli lia hrefhttpswwwtabulariotabularali lia hrefhttpswwwmicrosoftcommicrosoftali lia hrefhttpswwwdatabrickscomcompanyaboutusdatabricksali lia hrefhttpswwwonehouseaionehouseali lia hrefhttpsdeltaiodelta lakeali lidata warehouseli lia hrefhttpswwwsnowflakecomensnowflakeali lia hrefhttpswwwfireboltiofireboltali lidata qualityli lia hrefhttpswwwgetdbtcomdbtali lia hrefhttpswwwgableaigableali lia hrefhttpswwwgreatexpectationsiogreat expectationsali lia hrefhttpsstreamdalcomstreamdalali lia hrefhttpscoalesceiocoalesceali lia hrefhttpswwwsodaiosodaali lia hrefhttpsdqopscomdqopsali lieducation companiesli lia hrefhttpswwwdataexpertiodataexpertioali lia hrefhttpswwwlearndataengineeringcomlearndataengineeringcomali lia hrefhttpswwwalgoexpertioalgoexpertali lia hrefhttpswwwbytebytegocombytebytegoali lianalytics visualizationli lia hrefhttpswwwpresetiopresetali lia hrefhttpswwwstarburstiostarburstali lia hrefhttpswwwmetabasecommetabaseali lia hrefhttpslookerstudiogooglecomoverviewlooker studioali lia hrefhttpswwwtableaucomtableauali lia hrefhttpspowerbimicrosoftcompower biali lia hrefhttpssupersetapacheorgapache supersetali lidata integrationli lia hrefhttpscubedevcubeali lia hrefhttpswwwfivetrancomfivetranali lia hrefhttpsairbyteioairbyteali lia hrefhttpsdlthubcomdltali lia hrefhttpsslingdataioslingali lia hrefhttpsmeltanocommeltanoali limodern olapli lia hrefhttpsdruidapacheorgapache druidali lia hrefhttpsclickhousecomclickhouseali lia hrefhttpspinotapacheorgapache pinotali lia hrefhttpskylinapacheorgapache kylinali lia hrefhttpsduckdborgduckdbali lillm application libraryli lia hrefhttpsgithubcomsylphaiincadalflowadalflowali lia hrefhttpsgithubcomlangchainailangchainlangchainali lia hrefhttpsgithubcomrunllamallamaindexllamaindexali lirealtime datali lia hrefhttpsaggregationsioaggregationsioali lia hrefhttpswwwresponsivedevresponsiveali lia hrefhttpsrisingwavecomrisingwaveali ul h3data engineering blogs of companiesh3 ul lia hrefhttpsnetflixtechblogcomtaggedbigdatanetflixali lia hrefhttpswwwubercombloghoustondatauclickidb2f43229f3f44baebd5d10a05db2f70cuberali lia hrefhttpswwwdatabrickscomblogcategoryengineeringdataengineeringdatabricksali lia hrefhttpsmediumcomairbnbengineeringdatahomeairbnbali lia hrefhttpsawsamazoncomblogsbigdataamazon aws blogali lia hrefhttpstechcommunitymicrosoftcomt5dataarchitectureblogbgpdataarchitectureblogmicrosoft data architecture blogsali lia hrefhttpsblogfabricmicrosoftcommicrosoft fabric blogali lia hrefhttpsblogsoraclecomdatawarehousingoracleali lia hrefhttpsengineeringfbcomcategorydatainfrastructuremetaali lia hrefhttpswwwonehouseaiblogonehouseali ul h3data engineering whitepapersh3 ul lia hrefhttpsibimapublishingcomarticlescibima2011695619695619pdfa fivelayered business intelligence architectureali lia hrefhttpswwwcidrdborgcidr2021paperscidr2021paper17pdflakehousea new generation of open platforms that unify data warehousing and advanced analyticsali lia hrefhttpslinkspringercomchapter10100797830302338155big data quality a data quality profiling modelali lia hrefhttpsarxivorgabs231008697the data lakehouse data warehousing and moreali lia hrefhttpsdlacmorgdoi10555518631031863113spark cluster computing with working setsali lia hrefhttpsresearchgooglepubsthegooglefilesystemthe google file systemali lia hrefhttpswwwonehouseaiwhitepaperonehouseuniversaldatalakehousewhitepaperbuilding a universal data lakehouseali lia hrefhttpsarxivorgabs240109621xtable in action seamless interoperability in data lakesali lia hrefhttpsresearchgooglepubsmapreducesimplifieddataprocessingonlargeclustersmapreduce simplified data processing on large clustersali ul h3great youtube channelsh3 ul li pemyou have to have gt10k subscribes to be addedemp li li p100k subscribersp li lia hrefhttpswwwyoutubecomshashankmishraelearning bridgeali lia hrefhttpswwwyoutubecomcseattledataguyseattle data guyali lia hrefhttpswwwyoutubecomctrendytechinsightstrendytechali lia hrefhttpswwwyoutubecomdarshilparmardarshil parmarali lia hrefhttpswwwyoutubecomcandreaskayyandreas kretzali lia hrefhttpswwwyoutubecomcbytebytegobytebytegoali lia hrefhttpsyoutubecomtheravitshowthe ravit showali lia hrefhttpswwwyoutubecomguyinacubeguy in a cubeali lia hrefhttpswwwyoutubecomadammarczakytadam marczakali lia hrefhttpswwwyoutubecomnullqueriesnullqueriesali lia hrefhttpswwwyoutubecomtechtfqtechtfq by thoufiqali lia hrefhttpswwwyoutubecomsqlbisqlbiali li10k subscribersli lia hrefhttpswwwyoutubecomcdatawithzachdata with zachali lia hrefhttpswwwyoutubecomazurelibacademyazure libali lia hrefhttpswwwyoutubecomadvancinganalyticsadvancing analyticsali lia hrefhttpswwwyoutubecomkahandatasolutionskahan data solutionsali lia hrefhttpsyoutubecomankitbansal6ankit bansalali lia hrefhttpswwwyoutubecomchanneluczdoan4amf65pmllks8lmwwmr k talks techali ul h3linkedin voicesh3 ul li pemyou"}, {"filename": "README.md", "content": "have to have gt5k followers to be addedemp li li p100k followersp li lia hrefhttpswwwlinkedincomineczachlyzach wilsonali lia hrefhttpswwwlinkedincominbenjaminrogojanben rogojanali lia hrefhttpswwwlinkedincominbigdatabysumitsumit mittalali lia hrefhttpswwwlinkedincominshashank219shashank mishraali lia hrefhttpswwwlinkedincominchiphuyenchip huyenali lia hrefhttpswwwlinkedincominalexxubytealex xuali lia hrefhttpswwwlinkedincomindeepakgoyal93805a17deepak goyalali lia hrefhttpswwwlinkedincominandreaskretzandreas kretzali li50k followersli lia hrefhttpswwwlinkedincominjosephreisjoe reisali lia hrefhttpswwwlinkedincomindarshilparmardarshil parmarali lia hrefhttpswwwlinkedincominankitbansal6ankit bansalali lia hrefhttpswwwlinkedincominmarclambertimarc lambertiali lia hrefhttpswwwlinkedincominsqlbimarco russoali li10k followersli lia hrefhttpswwwlinkedincominliyinaili yinali lia hrefhttpswwwlinkedincominjosephmachado1991joseph machadoali lia hrefhttpswwwlinkedincomincodingwithrobyeric robyali lia hrefhttpswwwlinkedincominsimonwhiteleyuksimon whiteleyali lia hrefhttpswwwlinkedincominsspaetisimon sp\u00e4tiali li5k followersli lia hrefhttpswwwlinkedincomindipankarmazumdardipankar mazumdarali lia hrefhttpswwwlinkedincomindanielciocirlandaniel ciocirlanali lia hrefhttpswwwlinkedincominhugoluconfirmedhugo luali lia hrefhttpswwwlinkedincomintmaceytobias maceyali lia hrefhttpswwwlinkedincominmlortizmarcos ortizali lia hrefhttpswwwlinkedincominjulienhuraultanalyticsjulien huraultali ul h3twitter x voicesh3 ul li pemyou have to have gt5k followers to be addedemp li li p100k followersp li lia hrefhttpstwittercomalexxubytealex xuali li10k followersli lia hrefhttpswwwtwittercomeczachlyzach wilsonali lia hrefhttpswwwtwittercomseattledataguyseattle data guyali lia hrefhttpsxcommarcorusmarco russoali lia hrefhttpswwwtwittercomdanielblancoswedaniel blancoali li5k followersli lia hrefhttpswwwtwittercombigdatasumitsumit mittalali lia hrefhttpstwittercomstartdataengjoseph machadoali ul h3instagram creatorsh3 ul li pemyou have to have gt5k followers to be addedemp li li p100k followersp li lia hrefhttpswwwinstagramcomeczachlyzach wilsonali li5k followersli lia hrefhttpswwwinstagramcomlearndataengineeringandreas kretzali ul ptiktok emyou have to have gt10k followers to be addedemp ul li50k followersli lia hrefhttpswwwtiktokcomeczachlyzach wilsonali li10k followersli lia hrefhttpswwwtiktokcomalextheanalystalex the analystali ul h3great podcastsh3 ul lia hrefhttpswwwdataengineeringshowcomthe data engineering showali lia hrefhttpswwwdataengineeringpodcastcomdata engineering podcastali lia hrefhttpswwwdatatopicsiodatatopicsali lia hrefhttpspodcastsapplecomuspodcasttheengineeringsideofdataid1566999533the data engineering side of dataali lia hrefhttpswwwascendiodataawarepodcastdatawareali lia hrefhttpswwwdeezercomusshow5293247the data coffee break podcastali lia hrefhttpsdatastackshowcomthe datastack showali lia hrefhttpswwwintricitycomlearningcenterpodcastintricity101 data sharks podcastali lia hrefhttpswwwrittmananalyticscomdrilltodetaildrill to detail with mark rittmanali lia hrefhttpsanalyticshourioanalytics power hourali lia hrefhttpslistencasteduspublic127catalog26cocktails2fcf8728catalog amp cocktailsali lia hrefhttpsdatatalksclubpodcasthtmldatatalksali lia hrefhttpswwwdatabrickscomdiscoverdatabrewdata brew by databricksali lia hrefhttpsriseofthedatacloudsimplecastcomthe data cloud podcast by snowflakeali lia hrefhttpswwwstriimcompodcastwhats new in dataali lia hrefhttpswwwdatastaxcomresourcespodcastopensourcedataopensourcedata by datastaxali lia hrefhttpsdeveloperconfluentiopodcaststreaming audio by confluentali lia hrefhttpspodcastsapplecomuspodcastthedatascientistshowid1584430381the data scientist showali lia hrefhttpspodcastmlopscommunitymlopscommunityali lia hrefhttpsopenspotifycomshow3km3lbnzjpc1notjutbtmhmonday morning data chatali lia hrefhttpswwwthoughtspotcomdatachiefpodcastthe data chiefali ul h3great a hrefnewslettersmdlist of 20 newslettersah3 ptop must follow newsletters for data engineering a hrefhttpsblogdataengineeriodataengineerio newslettera a hrefhttpsjoereissubstackcomjoe reisa a hrefhttpswwwstartdataengineeringcomstart data engineeringa a hrefhttpswwwdataengineeringweeklycomdata engineering weeklyap h3glossariesh3 ul lia hrefhttpswwwsspshbraindataengineeringdata engineering vaultali lia hrefhttpsglossaryairbytecomairbyte data glossaryali lia hrefhttpsdataengineeringwikiindexdata engineering wiki by redditali lia hrefhttpswwwsecodacoglossaryseconda glossaryali lia hrefhttpswwwdatabrickscomglossaryglossary databricksali lia hrefhttpsairtablecomshrgh8bqzbkfkbrfktbluz3aylhc3cksdbairtable glossaryali lia hrefhttpsdagsterioglossarydata engineering glossary by dagsterali ul h3design patternsh3 ul lia hrefhttpswwwgithubcomdataexpertiocumulativetabledesigncumulative table designali lia hrefhttpswwwgithubcomeczachlymicrobatchhourlydedupedtutorialmicrobatch deduplicationali lia hrefhttpswwwgithubcomeczachlylittlebookofpipelinesthe little book of pipelinesali lia hrefhttpsdatadeveloperplatformorgarchitecturedata developer platformali ul h3courses academiesh3 ul lia hrefhttpswwwdataexpertiodataexpertio coursea use code stronghandbook10strong for a discountli lia hrefhttpswwwlearndataengineeringcomlearndataengineeringcomali lia hrefhttpswwwtechnicalfreelanceracademycomtechnical freelancer academya use code strongzwtechstrong for a discountli lia hrefhttpswwwedxorglearndataengineeringibmdataengineeringbasicsforeveryoneibm data engineering for everyoneali lia hrefhttpswwwqwiklabscomqwiklabsali lia hrefhttpswwwdatacampcomdatacampali lia hrefhttpswwwudemycomusershrutimantri5udemy courses from shruti mantriali lia hrefhttpsrockthejvmcomrock the jvma teaches spark in scala flink and othersli lia hrefhttpsdatatalksclubdata engineering zoomcamp by datatalksclubali lia hrefhttpsjosephmachadopodiacomefficientdataprocessinginsparkefficient data processing in sparkali lia hrefhttpswwwscalercomscalerali lia hrefhttpswwwdatateamsaidatateams data engingeer hiring platformali lia hrefhttpsdanielblancodevlinksudemy courses from daniel blancoali ul h3certifications coursesh3 ul lia hrefhttpscloudgooglecomcertificationdataengineergoogle cloud certified professional data engineerali lia hrefhttpswwwdatabrickscomlearncertificationdataengineerprofessionaldatabricks data engineer professionalali lia hrefhttpslearnmicrosoftcomcredentialscertificationsazuredataengineerazure data engineer associateali lia hrefhttpslearnmicrosoftcomcredentialscertificationsfabricanalyticsengineerassociatemicrosoft fabric analytics engineer associateali lia hrefhttpslearnmicrosoftcomenuscredentialscertificationsexamsdp203tabtablearningpathsexam dp203 data engineering on microsoft azureali lia hrefhttpsawsamazoncomcertificationcertifieddataengineerassociateaws certified data engineer associateali ul"}, {"filename": "professional_data_engineer_exam_guide_english (1).pdf", "content": "professional data engineer certification exam guide a professional data engineer makes data usable and valuable for others by collecting transforming and publishing data this individual evaluates and selects products and services to meet business and regulatory requirements a professional data engineer creates and manages robust data processing systems this includes the ability to design build deploy monitor maintain and secure data processing workloads section 1 designing data processing systems 22 of the exam 11 designing for security and compliance considerations include \u25cf identity and access management eg cloud iam and organization policies \u25cf data security encryption and key management \u25cf privacy eg personally identi\u0000able information and cloud data loss prevention api \u25cf regional considerations data sovereignty for data access and storage \u25cf legal and regulatory compliance 12 designing for reliability and \u0000delity considerations include \u25cf preparing and cleaning data eg dataprep data\u0000ow and cloud data fusion \u25cf monitoring and orchestration of data pipelines \u25cf disaster recovery and fault tolerance \u25cf making decisions related to acid atomicity consistency isolation and durability compliance and availability \u25cf data validation 13 designing for \u0000exibility and portability considerations include \u25cf mapping current and future business requirements to the architecture \u25cf designing for data and application portability eg multicloud and data residency requirements \u25cf data staging cataloging and discovery data governance 14 designing data migrations considerations include 1 \u25cf analyzing current stakeholder needs users processes and technologies and creating a plan to get to desired state \u25cf planning migration to google cloud eg bigquery data transfer service database migration service transfer appliance google cloud networking datastream \u25cf designing the migration validation strategy \u25cf designing the project dataset and table architecture to ensure proper data governance section 2 ingesting and processing the data 25 of the exam 21 planning the data pipelines considerations include \u25cf de\u0000ning data sources and sinks \u25cf de\u0000ning data transformation logic \u25cf networking fundamentals \u25cf data encryption 22 building the pipelines considerations include \u25cf data cleansing \u25cf identifying the services eg data\u0000ow apache beam dataproc cloud data fusion bigquery pubsub apache spark hadoop ecosystem and apache ka\u0000a \u25cf transformations \u25cb batch \u25cb streaming eg windowing late arriving data \u25cb language \u25cb ad hoc data ingestion onetime or automated pipeline \u25cf data acquisition and import \u25cf integrating with new data sources 23 deploying and operationalizing the pipelines considerations include \u25cf job automation and orchestration eg cloud composer and work\u0000ows \u25cf cicd continuous integration and continuous deployment section 3 storing the data 20 of the exam 31 selecting storage systems considerations include 2 \u25cf analyzing data access pa\u0000erns \u25cf choosing managed services eg bigtable spanner cloud sql cloud storage firestore memorystore \u25cf planning for storage costs and performance \u25cf lifecycle management of data 32 planning for using a data warehouse considerations include \u25cf designing the data model \u25cf deciding the degree of data normalization \u25cf mapping business requirements \u25cf de\u0000ning architecture to support data access pa\u0000erns 33 using a data lake considerations include \u25cf managing the lake con\u0000guring data discovery access and cost controls \u25cf processing data \u25cf monitoring the data lake 34 designing for a data"}, {"filename": "professional_data_engineer_exam_guide_english (1).pdf", "content": "mesh considerations include \u25cf building a data mesh based on requirements by using google cloud tools eg dataplex data catalog bigquery cloud storage \u25cf segmenting data for distributed team usage \u25cf building a federated governance model for distributed data systems section 4 preparing and using data for analysis 15 of the exam 41 preparing data for visualization considerations include \u25cf connecting to tools \u25cf precalculating \u0000elds \u25cf bigquery materialized views view logic \u25cf determining granularity of time data \u25cf troubleshooting poor performing queries \u25cf identity and access management iam and cloud data loss prevention cloud dlp 42 sharing data considerations include 3 \u25cf de\u0000ning rules to share data \u25cf publishing datasets \u25cf publishing reports and visualizations \u25cf analytics hub 43 exploring and analyzing data considerations include \u25cf preparing data for feature engineering training and serving machine learning models \u25cf conducting data discovery section 5 maintaining and automating data workloads 18 of the exam 51 optimizing resources considerations include \u25cf minimizing costs per required business need for data \u25cf ensuring that enough resources are available for businesscritical data processes \u25cf deciding between persistent or jobbased data clusters eg dataproc 52 designing automation and repeatability considerations include \u25cf creating directed acyclic graphs dags for cloud composer \u25cf scheduling jobs in a repeatable way 53 organizing workloads based on business requirements considerations include \u25cf flex ondemand and \u0000at rate slot pricing index on \u0000exibility or \u0000xed capacity \u25cf interactive or batch query jobs 54 monitoring and troubleshooting processes considerations include \u25cf observability of data processes eg cloud monitoring cloud logging bigquery admin panel \u25cf monitoring planned usage \u25cf troubleshooting error messages billing issues and quotas \u25cf manage workloads such as jobs queries and compute capacity reservations 55 maintaining awareness of failures and mitigating impact considerations include 4 \u25cf designing system for fault tolerance and managing restarts \u25cf running jobs in multiple regions or zones \u25cf preparing for data corruption and missing data \u25cf data replication and failover eg cloud sql redis clusters 5"}]